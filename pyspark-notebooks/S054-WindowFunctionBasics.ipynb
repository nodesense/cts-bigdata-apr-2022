{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ce193f-20ec-4532-baa4-8af2ea837e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dddc3b0-f19b-4818-9e1e-375a1a3e022e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 00:12:24 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.174.129 instead (on interface ens33)\n",
      "22/06/14 00:12:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.1.3-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e1d198b8-1982-4497-bcc3-c90ed2cfa2c2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.3 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.3 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 3486ms :: artifacts dl 43ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.3 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.3 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e1d198b8-1982-4497-bcc3-c90ed2cfa2c2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/41ms)\n",
      "22/06/14 00:12:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/14 00:12:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/06/14 00:12:50 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/06/14 00:12:50 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "config = SparkConf()\n",
    "# config.set(\"property\", \"value\")\n",
    "config.setMaster(\"local[4]\").setAppName(\"WindowFunction\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# spark Session, entry point for Spark SQL, DataFrame\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(conf=config)\\\n",
    "                    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c425b3ff-09ab-48a1-95f4-c16cd2e3377d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+\n",
      "|   name|     dept|salary|\n",
      "+-------+---------+------+\n",
      "|  James|    Sales|  3000|\n",
      "|Michael|    Sales|  4600|\n",
      "| Robert|    Sales|  4100|\n",
      "|  Maria|  Finance|  3000|\n",
      "|  James|    Sales|  3000|\n",
      "|  Scott|  Finance|  3300|\n",
      "|    Jen|  Finance|  3900|\n",
      "|   Jeff|Marketing|  3000|\n",
      "|  Kumar|Marketing|  2000|\n",
      "|   Saif|    Sales|  4100|\n",
      "|    Joe|    Sales|  4200|\n",
      "| Venkat|    Sales|  4000|\n",
      "+-------+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [ (\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100),\n",
    "    (\"Joe\", \"Sales\", 4200),\n",
    "    (\"Venkat\", \"Sales\", 4000),\n",
    "        \n",
    "   ]\n",
    "\n",
    "empDf = spark.createDataFrame(data=data, schema=['name', 'dept', 'salary'])\n",
    "empDf.printSchema()\n",
    "empDf.show()\n",
    "\n",
    "empDf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "521942e7-d99a-4b8d-b5fc-a71d7f77b700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[Row(name='James', dept='Sales', salary=3000),\n",
       "  Row(name='Michael', dept='Sales', salary=4600),\n",
       "  Row(name='Robert', dept='Sales', salary=4100)],\n",
       " [Row(name='Maria', dept='Finance', salary=3000),\n",
       "  Row(name='James', dept='Sales', salary=3000),\n",
       "  Row(name='Scott', dept='Finance', salary=3300)],\n",
       " [Row(name='Jen', dept='Finance', salary=3900),\n",
       "  Row(name='Jeff', dept='Marketing', salary=3000),\n",
       "  Row(name='Kumar', dept='Marketing', salary=2000)],\n",
       " [Row(name='Saif', dept='Sales', salary=4100),\n",
       "  Row(name='Joe', dept='Sales', salary=4200),\n",
       "  Row(name='Venkat', dept='Sales', salary=4000)]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empDf.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91fc9008-817b-4380-b78d-0c4c37d78202",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/home/ubuntu/employees already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mempDf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdept\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/ubuntu/employees\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py:1372\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression, sep\u001b[38;5;241m=\u001b[39msep, quote\u001b[38;5;241m=\u001b[39mquote, escape\u001b[38;5;241m=\u001b[39mescape, header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   1366\u001b[0m                nullValue\u001b[38;5;241m=\u001b[39mnullValue, escapeQuotes\u001b[38;5;241m=\u001b[39mescapeQuotes, quoteAll\u001b[38;5;241m=\u001b[39mquoteAll,\n\u001b[1;32m   1367\u001b[0m                dateFormat\u001b[38;5;241m=\u001b[39mdateFormat, timestampFormat\u001b[38;5;241m=\u001b[39mtimestampFormat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1370\u001b[0m                charToEscapeQuoteEscaping\u001b[38;5;241m=\u001b[39mcharToEscapeQuoteEscaping,\n\u001b[1;32m   1371\u001b[0m                encoding\u001b[38;5;241m=\u001b[39mencoding, emptyValue\u001b[38;5;241m=\u001b[39memptyValue, lineSep\u001b[38;5;241m=\u001b[39mlineSep)\n\u001b[0;32m-> 1372\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark-3.1.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark-3.1.3-bin-hadoop2.7/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/home/ubuntu/employees already exists."
     ]
    }
   ],
   "source": [
    "empDf.write.option(\"header\", True)\\\n",
    "  .partitionBy(\"dept\")\\\n",
    "  .csv(\"/home/ubuntu/employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0b033-393e-4f84-93ec-affc97b7340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# specification for window, partitions, functions that should be applied on partition\n",
    "# with in department, order the data based on salary in ascending order\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "# we have apply the spec on dataframe\n",
    "df = empDf.withColumn(\"slno\", row_number().over(windowSpec))\n",
    "\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "df.filter (df.slno == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aafacf1-81b9-45eb-947e-b565ee7df6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "# rank with gap with ascending order\n",
    "\"\"\"\n",
    "score  rank\n",
    "90      1\n",
    "90      1\n",
    "89      3  [gap, 2 not included]\n",
    "\"\"\"\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "\n",
    "df = empDf.withColumn(\"rank\", rank().over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29a8e418-2cfe-4e09-a613-5e89f5065560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:====================================================>  (96 + 4) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+-----+----+\n",
      "|   name|     dept|salary|first|last|\n",
      "+-------+---------+------+-----+----+\n",
      "|  James|    Sales|  3000| 3000|4600|\n",
      "|  James|    Sales|  3000| 3000|4600|\n",
      "| Venkat|    Sales|  4000| 3000|4600|\n",
      "| Robert|    Sales|  4100| 3000|4600|\n",
      "|   Saif|    Sales|  4100| 3000|4600|\n",
      "|    Joe|    Sales|  4200| 3000|4600|\n",
      "|Michael|    Sales|  4600| 3000|4600|\n",
      "|  Maria|  Finance|  3000| 3000|3900|\n",
      "|  Scott|  Finance|  3300| 3000|3900|\n",
      "|    Jen|  Finance|  3900| 3000|3900|\n",
      "|  Kumar|Marketing|  2000| 2000|3000|\n",
      "|   Jeff|Marketing|  3000| 2000|3000|\n",
      "+-------+---------+------+-----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(\"salary\")\\\n",
    "                  .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "\n",
    "df = empDf.withColumn(\"first\", first(\"salary\").over(windowSpec))\\\n",
    "          .withColumn(\"last\", last(\"salary\").over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf05a71a-11af-46eb-91bc-f643e08aca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, desc\n",
    "\n",
    "# rank with gap\n",
    "\"\"\"\n",
    "score  rank\n",
    "90      1\n",
    "90      1\n",
    "89      3  [gap, 2 not included]\n",
    "\"\"\"\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(desc(\"salary\"))\n",
    "\n",
    "df = empDf.withColumn(\"rank\", rank().over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25768961-8cf4-4a9b-a998-c2402b4e6ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank, desc\n",
    "\n",
    "# dense_rank ranking without gap\n",
    "\"\"\"\n",
    "score  rank\n",
    "90      1\n",
    "90      1\n",
    "89      2  \n",
    "\"\"\"\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(desc(\"salary\"))\n",
    "\n",
    "df = empDf.withColumn(\"rank\", dense_rank().over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6974943e-5533-4c62-9368-276660e0da96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:====================================================>  (95 + 5) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+-------------------+\n",
      "|   name|     dept|salary|               rank|\n",
      "+-------+---------+------+-------------------+\n",
      "|Michael|    Sales|  4600|                0.0|\n",
      "|    Joe|    Sales|  4200|0.16666666666666666|\n",
      "| Robert|    Sales|  4100| 0.3333333333333333|\n",
      "|   Saif|    Sales|  4100| 0.3333333333333333|\n",
      "| Venkat|    Sales|  4000| 0.6666666666666666|\n",
      "|  James|    Sales|  3000| 0.8333333333333334|\n",
      "|  James|    Sales|  3000| 0.8333333333333334|\n",
      "|    Jen|  Finance|  3900|                0.0|\n",
      "|  Scott|  Finance|  3300|                0.5|\n",
      "|  Maria|  Finance|  3000|                1.0|\n",
      "|   Jeff|Marketing|  3000|                0.0|\n",
      "|  Kumar|Marketing|  2000|                1.0|\n",
      "+-------+---------+------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import percent_rank, desc\n",
    "\n",
    "# percent_rank ranking with perecent calculation\n",
    "\"\"\"\n",
    " \n",
    "\"\"\"\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(desc(\"salary\"))\n",
    "\n",
    "df = empDf.withColumn(\"rank\", percent_rank().over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18f4ad2-73d1-40ac-be6c-78bb81416a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=====================================================> (98 + 2) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+----+\n",
      "|   name|     dept|salary|rank|\n",
      "+-------+---------+------+----+\n",
      "|Michael|    Sales|  4600|   1|\n",
      "|    Joe|    Sales|  4200|   1|\n",
      "| Robert|    Sales|  4100|   2|\n",
      "|   Saif|    Sales|  4100|   2|\n",
      "| Venkat|    Sales|  4000|   3|\n",
      "|  James|    Sales|  3000|   3|\n",
      "|  James|    Sales|  3000|   4|\n",
      "|    Jen|  Finance|  3900|   1|\n",
      "|  Scott|  Finance|  3300|   2|\n",
      "|  Maria|  Finance|  3000|   3|\n",
      "|   Jeff|Marketing|  3000|   1|\n",
      "|  Kumar|Marketing|  2000|   2|\n",
      "+-------+---------+------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import ntile, desc\n",
    "\n",
    "# ntile ranking with related certain range for range\n",
    "# rank shall fit into a range\n",
    "\"\"\"\n",
    " \n",
    "\"\"\"\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(desc(\"salary\"))\n",
    "\n",
    "df = empDf.withColumn(\"rank\", ntile(4).over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75d9e3ac-5265-49d0-b94d-ee5bd3c8833b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=================================================>     (90 + 5) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+-------------------+\n",
      "|   name|     dept|salary|          cume_dist|\n",
      "+-------+---------+------+-------------------+\n",
      "|Michael|    Sales|  4600|0.14285714285714285|\n",
      "|    Joe|    Sales|  4200| 0.2857142857142857|\n",
      "| Robert|    Sales|  4100| 0.5714285714285714|\n",
      "|   Saif|    Sales|  4100| 0.5714285714285714|\n",
      "| Venkat|    Sales|  4000| 0.7142857142857143|\n",
      "|  James|    Sales|  3000|                1.0|\n",
      "|  James|    Sales|  3000|                1.0|\n",
      "|    Jen|  Finance|  3900| 0.3333333333333333|\n",
      "|  Scott|  Finance|  3300| 0.6666666666666666|\n",
      "|  Maria|  Finance|  3000|                1.0|\n",
      "|   Jeff|Marketing|  3000|                0.5|\n",
      "|  Kumar|Marketing|  2000|                1.0|\n",
      "+-------+---------+------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Analytic functions\n",
    "# Cumulative distribution - similar to rank, calcualted and values are bound between \n",
    "# 0 and 1\n",
    "\n",
    "# 10 USD per share => 13 USD per share      = 3 USD per share, 30 % gain .3\n",
    "# 100 USD per share => 110 USD per share    = 10 USD per share, 10% gain .1\n",
    "# cumulative distribution\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import cume_dist, desc\n",
    "\n",
    "# similar to  rank  \n",
    " \n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(desc(\"salary\"))\n",
    "\n",
    "df = empDf.withColumn(\"cume_dist\", cume_dist().over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46a88ad0-5a46-4ac2-88c7-a37d13fcd84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:====================================================>  (95 + 4) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+----+\n",
      "|   name|     dept|salary| lag|\n",
      "+-------+---------+------+----+\n",
      "|  James|    Sales|  3000|null|\n",
      "|  James|    Sales|  3000|3000|\n",
      "| Venkat|    Sales|  4000|3000|\n",
      "| Robert|    Sales|  4100|4000|\n",
      "|   Saif|    Sales|  4100|4100|\n",
      "|    Joe|    Sales|  4200|4100|\n",
      "|Michael|    Sales|  4600|4200|\n",
      "|  Maria|  Finance|  3000|null|\n",
      "|  Scott|  Finance|  3300|3000|\n",
      "|    Jen|  Finance|  3900|3300|\n",
      "|  Kumar|Marketing|  2000|null|\n",
      "|   Jeff|Marketing|  3000|2000|\n",
      "+-------+---------+------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# lag - previous lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, desc\n",
    "\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "\n",
    "df = empDf.withColumn(\"lag\", lag(\"salary\",1).over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd885665-b210-4972-83be-725c1e158413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+----+\n",
      "|   name|     dept|salary|lead|\n",
      "+-------+---------+------+----+\n",
      "|  James|    Sales|  3000|3000|\n",
      "|  James|    Sales|  3000|4000|\n",
      "| Venkat|    Sales|  4000|4100|\n",
      "| Robert|    Sales|  4100|4100|\n",
      "|   Saif|    Sales|  4100|4200|\n",
      "|    Joe|    Sales|  4200|4600|\n",
      "|Michael|    Sales|  4600|null|\n",
      "|  Maria|  Finance|  3000|3300|\n",
      "|  Scott|  Finance|  3300|3900|\n",
      "|    Jen|  Finance|  3900|null|\n",
      "|  Kumar|Marketing|  2000|3000|\n",
      "|   Jeff|Marketing|  3000|null|\n",
      "+-------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lead -  the one ahead, \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead, desc\n",
    "\n",
    "windowSpec = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "\n",
    "df = empDf.withColumn(\"lead\", lead(\"salary\", 1).over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e017e-0330-4190-b48c-a00c643c0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate functions, min, max, sum, count, avg\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, sum, min, max, count, col\n",
    "\n",
    "windowSpec = Window.partitionBy(\"dept\")\n",
    "\n",
    "df = empDf\\\n",
    "          .withColumn(\"min\", min(col(\"salary\")).over(windowSpec))\\\n",
    "          .withColumn(\"max\", max(col(\"salary\")).over(windowSpec))\\\n",
    "          .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpec))\\\n",
    "          .withColumn(\"count\", count(col(\"salary\")).over(windowSpec))\\\n",
    "          .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpec))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b3e4f-101e-413f-8d1b-917482a8347e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
