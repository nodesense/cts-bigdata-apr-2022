{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cb509ee-e25d-47c4-b65e-5cab1d6ba36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is centralized in hadoop\n",
    "# metadata is local to spark notebook\n",
    "# not scalable, good for learning setup without hive in central location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d40df5-dc32-4428-a1aa-0b8ced579488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example. using local meta store, using hdfs to store data\n",
    "# meta store is not usable in other notebooks, as it is embedeed locked jvm internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad36b20c-c8f6-4c31-bbd3-0ee46f59d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ee21e3-b5f7-4555-be07-2dcb6f2c16cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSpark Database \\nBasic Database:\\nNow we will HDFS location for spark temp data, also spark datawarehouse directory,\\n\\nhowever metastore db/meta data is embeeded inside spark, mean not usable for other application\\n\\n\\nIn production, you will be using Hive Data Catalog\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Spark Database \n",
    "Basic Database:\n",
    "Now we will HDFS location for spark temp data, also spark datawarehouse directory,\n",
    "\n",
    "however metastore db/meta data is embeeded inside spark, mean not usable for other application\n",
    "\n",
    "\n",
    "In production, you will be using Hive Data Catalog\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cd452ca-1f5b-4db4-bcdd-10ba1372221e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSpark HDFS Location\\nOnly for dev only, not for production\\n\\n3 components involved\\n\\n1. meta data - database name, tables, columns data types, location where data stored\\n    is managed by hive, hive internally uses derby db to store all meta data\\n2. spark temporary location  \"spark.local.dir\", \"hdfs://localhost:9000/spark-temp\"\\n    where temp data used internally stored\\n    \\n3. \"spark.sql.warehouse.dir\", \"hdfs://localhost:9000/spark-warehouse\" spark data warehouse where all the database data shall be stored\\n    we can see database, tables, their data where meta data ,table name, columns are stored in \\n    meta data\\n    \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Spark HDFS Location\n",
    "Only for dev only, not for production\n",
    "\n",
    "3 components involved\n",
    "\n",
    "1. meta data - database name, tables, columns data types, location where data stored\n",
    "    is managed by hive, hive internally uses derby db to store all meta data\n",
    "2. spark temporary location  \"spark.local.dir\", \"hdfs://localhost:9000/spark-temp\"\n",
    "    where temp data used internally stored\n",
    "    \n",
    "3. \"spark.sql.warehouse.dir\", \"hdfs://localhost:9000/spark-warehouse\" spark data warehouse where all the database data shall be stored\n",
    "    we can see database, tables, their data where meta data ,table name, columns are stored in \n",
    "    meta data\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72ae68f4-1e16-4676-90b5-7f13ae076722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 20:17:07 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.174.129 instead (on interface ens33)\n",
      "22/05/16 20:17:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/05/16 20:17:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/16 20:17:10 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "config = SparkConf()\n",
    "# config.set(\"property\", \"value\")\n",
    "config.setMaster(\"local\").setAppName(\"SparkDatabase\")\n",
    "\n",
    "# embedded, simple, local spark database/warehouse\n",
    "# spark will store temporary files\n",
    "# enable hive support must for sql database\n",
    "# enable hiveSupport hive catalog to be embedded inside working directory\n",
    "# spark temp data goes to \"hdfs://localhost:9000/spark-temp\"\n",
    "config.set(\"spark.local.dir\", \"/home/ubuntu/spark-temp\")\n",
    "# spark data [not meta data] goes into  \"/home/ubuntu/spark-warehouse\"\n",
    "config.set(\"spark.sql.warehouse.dir\", \"hdfs://localhost:9000/spark-warehouse\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# spark Session, entry point for Spark SQL, DataFrame\n",
    "\n",
    "# enableHiveSupport() create a meta catalog/database using derby database\n",
    "# inside current working directory, embedded into spark notebook,\n",
    "# multiple notebooks cannot share at same time.\n",
    "# inside pyspark-notebooks, you could see metastore_db\n",
    "# metastore shall have meta data: database, tables, columns, data types, where exactly\n",
    "# data located in hdfs or file system or s3\n",
    "# derby.log - derby database log \n",
    "## metastore_db \n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(conf=config)\\\n",
    "                    .enableHiveSupport()\\\n",
    "                    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5cc9b9c-da36-4561-b4f1-21251fe9798d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 20:18:36 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "22/05/16 20:18:36 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "22/05/16 20:18:46 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "22/05/16 20:18:46 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore ubuntu@127.0.1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|productdb|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from metadata\n",
    "df = spark.sql(\"SHOW DATABASES\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e9c5238-6383-4927-85d0-a42c9cefb6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 20:19:20 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "22/05/16 20:19:21 WARN ObjectStore: Failed to get database stocklocaldb, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta data local, but data directory should be in hdfs spark-warehouse\n",
    "# hdfs://localhost:9000/spark-warehouse/spark-warehouse/stocklocaldb.db\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS stocklocaldb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ac8a77-8ddf-4354-84d3-1aeb5e315570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 20:28:53 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "22/05/16 20:28:53 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "22/05/16 20:28:54 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "22/05/16 20:28:54 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "22/05/16 20:28:54 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "22/05/16 20:28:54 WARN HiveMetaStore: Location: hdfs://localhost:9000/spark-warehouse/stocklocaldb.db/stocks specified for non-external table:stocks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create spark Managed table\n",
    "# we have to use spark sql like insert, (update, delete won't work at 2.x)\n",
    "# to add data\n",
    "# create databsae called stocklocaldb\n",
    "# hdfs://localhost:9000/spark-warehouse/spark-warehouse/stocklocaldb.db/stocks\n",
    "spark.sql(\"CREATE TABLE  IF NOT EXISTS stocklocaldb.stocks(symbol STRING, industry STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65c14ab4-2f62-4e59-9df2-6fac11cc46b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 20:30:01 ERROR KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    " INSERT INTO stocklocaldb.stocks VALUES('INFY', 'IT')\n",
    "\"\"\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34f4d1f8-2132-4319-ae8b-aac9669fd2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|symbol|industry|\n",
      "+------+--------+\n",
      "|  INFY|      IT|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM stocklocaldb.stocks\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c68830da-af9f-41fc-9b3b-eed806079e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this command drop the table from meta data store and drop the in the \n",
    "# spark datawarehouse directory\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS stocklocaldb.stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fbe7dcf-c649-4c60-aef1-2e407ba55e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES in stocklocaldb\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f619048f-0f23-4ee1-be20-2337b50ed721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/16 20:33:40 WARN TxnHandler: Cannot perform cleanup since metastore table does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if no table exists, no data inside, then it drop the database\n",
    "# drop the metadata too..\n",
    "spark.sql(\"DROP DATABASE IF EXISTS stocklocaldb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e692673e-3d22-4433-bfc4-48b8a6f3149d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|productdb|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6ab7c-8f0c-4914-a27a-35f49abbcc12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
