{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41dbdef7-dcf7-4216-860f-d29aac7b2b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "990462a4-50ee-47a2-8545-d4dba6d45ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/05 21:41:02 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.174.129 instead (on interface ens33)\n",
      "22/05/05 21:41:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/05/05 21:41:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/05 21:41:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/05/05 21:41:10 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/05/05 21:41:10 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"ReadFromHDFS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d7df977-6879-4b3f-8f63-75d9f2322beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a text file from HDFS, movies.csv\n",
    "# hdfs dfs -ls /ml-latest-small\n",
    "rdd = sc.textFile(\"hdfs://localhost:9000/ml-latest-small/movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d23acad9-e2a3-416f-8819-6ce02a312570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9743"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count() # action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c7648c3-2077-4e14-a8ed-be60d53cb91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/05 21:48:58 WARN BlockReaderFactory: I/O error constructing remote block reader.\n",
      "java.nio.channels.ClosedByInterruptException\n",
      "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
      "\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3441)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:665)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1567)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:883)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:312)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:243)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "22/05/05 21:48:58 WARN DFSClient: Failed to connect to /127.0.0.1:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException\n",
      "java.nio.channels.ClosedByInterruptException\n",
      "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
      "\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3441)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:665)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1567)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:883)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:312)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:243)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "22/05/05 21:48:58 WARN DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 2823.8459339999945 msec.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'movieId,title,genres'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89934c2-f306-486e-89d4-5b7ebec36b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieId,title,genres\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/05 21:55:39 WARN BlockReaderFactory: I/O error constructing remote block reader.\n",
      "java.nio.channels.ClosedByInterruptException\n",
      "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
      "\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3441)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:665)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1567)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:883)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:312)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:243)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "22/05/05 21:55:39 WARN DFSClient: Failed to connect to /127.0.0.1:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException\n",
      "java.nio.channels.ClosedByInterruptException\n",
      "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
      "\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3441)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:665)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1567)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:883)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:312)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:243)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "22/05/05 21:55:39 WARN DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1884.0994209763396 msec.\n"
     ]
    }
   ],
   "source": [
    "# get first line\n",
    "result = rdd.first()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "040f9b88-81bc-46a9-892e-8835b0685a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movieId,title,genres', '1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy', '2,Jumanji (1995),Adventure|Children|Fantasy', '3,Grumpier Old Men (1995),Comedy|Romance', '4,Waiting to Exhale (1995),Comedy|Drama|Romance']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/05 21:56:22 WARN BlockReaderFactory: I/O error constructing remote block reader.\n",
      "java.nio.channels.ClosedByInterruptException\n",
      "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
      "\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3441)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:665)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1567)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:883)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:312)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:243)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "22/05/05 21:56:22 WARN DFSClient: Failed to connect to /127.0.0.1:50010 for block, add to deadNodes and continue. java.nio.channels.ClosedByInterruptException\n",
      "java.nio.channels.ClosedByInterruptException\n",
      "\tat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n",
      "\tat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:658)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3441)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:777)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:694)\n",
      "\tat org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:355)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:665)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.seekToBlockSource(DFSInputStream.java:1567)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:883)\n",
      "\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:926)\n",
      "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n",
      "\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\n",
      "\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:312)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:243)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\n",
      "22/05/05 21:56:22 WARN DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 654.4352655863233 msec.\n"
     ]
    }
   ],
   "source": [
    "print  ( rdd.take(5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cbee164-5dfc-4c90-ab39-880deca801d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy', '2,Jumanji (1995),Adventure|Children|Fantasy', '3,Grumpier Old Men (1995),Comedy|Romance', '4,Waiting to Exhale (1995),Comedy|Drama|Romance', '5,Father of the Bride Part II (1995),Comedy']\n"
     ]
    }
   ],
   "source": [
    "# to skip first line, \n",
    "headerLine = rdd.first()\n",
    "# no header data in rddContent\n",
    "rddContent = rdd.filter (lambda line: line != headerLine)\n",
    "\n",
    "print (rddContent.take(5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f95c2195-82a1-42f8-8211-85b403e67c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Toy Story (1995)\n",
      "Adventure|Animation|Children|Comedy|Fantasy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1', 'Toy Story (1995)', 'Adventure|Animation|Children|Comedy|Fantasy']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then each split line into list \n",
    "line = '1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy'\n",
    "out = line.split(\",\") # assignment\n",
    "\n",
    "print(out[0]) # ,movie id\n",
    "print(out[1]) # title\n",
    "print(out[2]) # genres\n",
    "\n",
    "out # expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5109ca21-caf8-4f70-8417-4c9654d89ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 'Toy Story (1995)', 'Adventure|Animation|Children|Comedy|Fantasy'), ('2', 'Jumanji (1995)', 'Adventure|Children|Fantasy'), ('3', 'Grumpier Old Men (1995)', 'Comedy|Romance'), ('4', 'Waiting to Exhale (1995)', 'Comedy|Drama|Romance'), ('5', 'Father of the Bride Part II (1995)', 'Comedy')]\n"
     ]
    }
   ],
   "source": [
    "# moviesRdd is list of list consist of elements parsed using ,\n",
    "# split returns list, we can convert into tuple\n",
    "\n",
    "moviesRdd = rddContent.map (lambda line: tuple(line.split(\",\")))\n",
    "\n",
    "print(moviesRdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e2979ca-7434-444b-9319-a98d9dd5cd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'TOY STORY (1995)', 'adventure|animation|children|comedy|fantasy'), (2, 'JUMANJI (1995)', 'adventure|children|fantasy'), (3, 'GRUMPIER OLD MEN (1995)', 'comedy|romance'), (4, 'WAITING TO EXHALE (1995)', 'comedy|drama|romance'), (5, 'FATHER OF THE BRIDE PART II (1995)', 'comedy')]\n"
     ]
    }
   ],
   "source": [
    "# convert movie id into int\n",
    "# convert title into upper case\n",
    "# convert genres into lower case\n",
    "moviesRdd2 = moviesRdd.map (lambda t: (int(t[0]), t[1].upper(), t[2].lower()) )\n",
    "print(moviesRdd2.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1750f2c4-0cc4-4297-9973-536c74b40967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100, 'CITY HALL (1996)', 'drama|thriller'), (101, 'BOTTLE ROCKET (1996)', 'adventure|comedy|crime|romance'), (102, 'MR. WRONG (1996)', 'comedy'), (103, 'UNFORGETTABLE (1996)', 'mystery|sci-fi|thriller'), (104, 'HAPPY GILMORE (1996)', 'comedy'), (105, '\"BRIDGES OF MADISON COUNTY', ' the (1995)\"')]\n"
     ]
    }
   ],
   "source": [
    "# DIY\n",
    "# use moviesRdd2 and filter, print the movies id  betwen 100 to 105\n",
    "filteredRdd = moviesRdd2.filter (lambda t: t[0] >= 100 and t[0] <= 105)\n",
    "print(filteredRdd.take(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f897d80-2010-4cd1-9710-a3fdd4177361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 'TOY STORY (1995)', 'adventure|animation|children|comedy|fantasy'),\n",
       " (3, 'GRUMPIER OLD MEN (1995)', 'comedy|romance'),\n",
       " (4, 'WAITING TO EXHALE (1995)', 'comedy|drama|romance'),\n",
       " (5, 'FATHER OF THE BRIDE PART II (1995)', 'comedy'),\n",
       " (7, 'SABRINA (1995)', 'comedy|romance'),\n",
       " (12, 'DRACULA: DEAD AND LOVING IT (1995)', 'comedy|horror'),\n",
       " (18, 'FOUR ROOMS (1995)', 'comedy'),\n",
       " (19, 'ACE VENTURA: WHEN NATURE CALLS (1995)', 'comedy'),\n",
       " (20, 'MONEY TRAIN (1995)', 'action|comedy|crime|drama|thriller'),\n",
       " (21, 'GET SHORTY (1995)', 'comedy|crime|thriller')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DIY print 10 movies of genres comedy, filter\n",
    "s = 'adventure|comedy|crime|romance'\n",
    "print('comedy' in s) # True\n",
    "print ('mystery' in s) # False\n",
    "filteredRdd =  moviesRdd2.filter (lambda t: 'comedy' in t[2])\n",
    "filteredRdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460f2079-21eb-4444-b079-2126e46d5687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
