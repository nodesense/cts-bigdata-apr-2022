{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f776bd4e-6323-4ea3-9d46-a3ae37b3e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ddb4da-61f9-4d73-9464-257b7c5af0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n0. download movie lens data https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\\nin ubuntu and extract content\\n\\n1. cd ~\\n2. mkdir ratings \\n3. cd ratings \\n4. mkdir inputs\\n5. code .\\n6. create file1.csv, file2.csv, file3.csv ... file10.csv in ratings directory\\n7. remove header from ratings.csv \\n8. cut 10 records and paste them into ratings/file1.csv folder\\n9. Do step 8 for all 10 files\\n10. We treat files as stream , some ETL system add files to directory\\n11. Run teh FileStream note book\\n12. Copy 1 file into inputs directory by drag drop\\n13. Check wehther spark able to process\\n14. Copy file  2 into inputs\\n15. stop spark notebook\\n16. copy file 3 and file 4, see spark read the files\\n17. stop the jupyter\\n18. Copy files again\\n19. start spark and check wehtehr spark process the data or not\\n20. stop spark \\n21. change checkpoint location\\n22. check whether all files get processed again or not\\n23. what is your understanding of checkpoints\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "0. download movie lens data https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "in ubuntu and extract content\n",
    "\n",
    "1. cd ~\n",
    "2. mkdir ratings \n",
    "3. cd ratings \n",
    "4. mkdir inputs\n",
    "5. code .\n",
    "6. create file1.csv, file2.csv, file3.csv ... file10.csv in ratings directory\n",
    "7. remove header from ratings.csv \n",
    "8. cut 10 records and paste them into ratings/file1.csv folder\n",
    "9. Do step 8 for all 10 files\n",
    "10. We treat files as stream , some ETL system add files to directory\n",
    "11. Run teh FileStream note book\n",
    "12. Copy 1 file into inputs directory by drag drop\n",
    "13. Check wehther spark able to process\n",
    "14. Copy file  2 into inputs\n",
    "15. stop spark notebook\n",
    "16. copy file 3 and file 4, see spark read the files\n",
    "17. stop the jupyter\n",
    "18. Copy files again\n",
    "19. start spark and check wehtehr spark process the data or not\n",
    "20. stop spark \n",
    "21. change checkpoint location\n",
    "22. check whether all files get processed again or not\n",
    "23. what is your understanding of checkpoints\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a1c3d5c-0b09-465a-a0f8-00df2b02d824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/09 21:56:09 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.174.129 instead (on interface ens33)\n",
      "22/06/09 21:56:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.1.3-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e6a5faaa-b295-4e44-a62f-b2dcd3ab9cdf;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.3 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.3 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 1279ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.3 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.3 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e6a5faaa-b295-4e44-a62f-b2dcd3ab9cdf\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/10ms)\n",
      "22/06/09 21:56:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "config = SparkConf()\n",
    "# config.set(\"property\", \"value\")\n",
    "config.setMaster(\"local\").setAppName(\"RatingStream\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# spark Session, entry point for Spark SQL, DataFrame\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(conf=config)\\\n",
    "                    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "240c942f-ceb0-49ae-b231-799d9ee49be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, LongType, StringType, IntegerType, DoubleType\n",
    "\n",
    "ratingSchema = StructType()\\\n",
    "                .add(\"userId\", IntegerType(), True)\\\n",
    "                .add(\"movieId\", IntegerType(), True)\\\n",
    "                .add(\"rating\", DoubleType(), True)\\\n",
    "                .add(\"timestamp\", LongType(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d81d7115-6a48-4901-abab-eab4c813479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /home/ubuntu/ratings/stream\n",
    "\n",
    "df = spark.readStream.schema(ratingSchema)\\\n",
    "                .csv(\"/home/ubuntu/ratings/inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55357c3c-a5d6-4b71-9d5a-d7d2445c5c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|    163|   5.0|964983650|\n",
      "|     1|    216|   5.0|964981208|\n",
      "|     1|    223|   3.0|964980985|\n",
      "|     1|    231|   5.0|964981179|\n",
      "|     1|    235|   4.0|964980908|\n",
      "|     1|    260|   5.0|964981680|\n",
      "|     1|    296|   3.0|964982967|\n",
      "|     1|    316|   3.0|964982310|\n",
      "|     1|    333|   5.0|964981179|\n",
      "|     1|    349|   4.0|964982563|\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "|     1|     70|   3.0|964982400|\n",
      "|     1|    101|   5.0|964980868|\n",
      "|     1|    110|   4.0|964982176|\n",
      "|     1|    151|   5.0|964984041|\n",
      "|     1|    157|   5.0|964984100|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "echoOnconsole = df\\\n",
    "                .writeStream\\\n",
    "                .outputMode(\"append\")\\\n",
    "                .format(\"console\")\\\n",
    "                 .option(\"checkpointLocation\", \"file:///tmp/spark568\")\\\n",
    "                .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b171abde-c37a-4def-b925-cb71a619ede7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
