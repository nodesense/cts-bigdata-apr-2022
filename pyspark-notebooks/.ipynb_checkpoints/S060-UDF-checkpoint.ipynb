{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd361d27-9481-4e05-954b-deea5e32c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca2fdefc-dccf-4104-9cd3-fe39db461fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/18 00:07:37 WARN Utils: Your hostname, ubuntu-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.174.129 instead (on interface ens33)\n",
      "22/05/18 00:07:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/05/18 00:07:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/18 00:07:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "config = SparkConf()\n",
    "# config.set(\"property\", \"value\")\n",
    "config.setMaster(\"local\").setAppName(\"UDF\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# spark Session, entry point for Spark SQL, DataFrame\n",
    "# in single spark driver/note book/spark application,\n",
    "# there can be many spark sessions, and \n",
    "# only one spark context\n",
    "spark = SparkSession.builder\\\n",
    "                    .config(conf=config)\\\n",
    "                    .getOrCreate()\n",
    "# spark core operations, like rdd, partitions, actions etc\n",
    "# spark session shall use catalyst engine, which will use spark context for low level\n",
    "# code execution\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f08197b7-73b1-494d-a73b-76bf2cbeb47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF - User Defined Functions, custom functions written in scala,java, python \n",
    "# usedin spakr sql, spark dataframe\n",
    "# python - slow\n",
    "# scala, java - fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dbeec53-d076-499a-bac2-9e23759ca727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/18 00:13:18 WARN SimpleFunctionRegistry: The function power replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(n)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "# UDF - User Defined Functions\n",
    "# useful to extend spark sql functions with custom code\n",
    "\n",
    "power = lambda n : n * n\n",
    "\n",
    "from pyspark.sql.functions import udf \n",
    "from pyspark.sql.types import LongType\n",
    "# create udf with return type\n",
    "powerUdf = udf(power, LongType())\n",
    "\n",
    "# we must register udf in spark session\n",
    "# udf too private within spark session, udf registered in spark session not avaialble in another spark session\n",
    "# \"power\" is udf function name, can be used sql\n",
    "spark.udf.register(\"power\", powerUdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa19c134-59bc-4fcf-bab8-cba53b840adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|power(5)|\n",
      "+--------+\n",
      "|      25|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# power is udf function\n",
    "spark.sql(\"SELECT power(5)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4c3f193-4556-425f-8d48-395bd1aa861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+-----+---+--------+----+\n",
      "|product_id|product_name|brand_id|price|qty|discount|taxp|\n",
      "+----------+------------+--------+-----+---+--------+----+\n",
      "|         1|      iPhone|     100| 1000|  2|       5|  18|\n",
      "|         2|      Galaxy|     200|  800|  1|       8|  22|\n",
      "+----------+------------+--------+-----+---+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "# Databricks notebook source\n",
    "orders = [ \n",
    "          # (product_id, product_name, brand_id, price, qty, discount, taxp)  \n",
    "         (1, 'iPhone', 100, 1000, 2, 5, 18),\n",
    "         (2, 'Galaxy', 200, 800, 1, 8, 22),\n",
    "\n",
    "]\n",
    " \n",
    "\n",
    "\n",
    "orderDf = spark.createDataFrame(data=orders, schema=[\"product_id\", \"product_name\", \"brand_id\", \"price\", \"qty\", \"discount\", \"taxp\"])\n",
    "orderDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4baf3216-d426-4384-8f9c-62aa1e21f546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount is 2242.0\n",
      "2242.0\n"
     ]
    }
   ],
   "source": [
    "# UDF to calculate amount\n",
    "# amount = ( price * qty ) * apply discount * taxp\n",
    "\n",
    "def calculateAmount(price, qty, discount, taxp):\n",
    "    a = price * qty\n",
    "    a = a - (a * discount/100) # discounted price\n",
    "    amount = a + a * taxp / 100 # with tax\n",
    "    print (\"amount is\" , amount) \n",
    "    return amount\n",
    "\n",
    "print(calculateAmount(1000, 2, 5, 18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eadd42c-b841-4de9-9699-4c5b8f2059c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.calculateAmount(price, qty, discount, taxp)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "# udf function\n",
    "calculate = udf(calculateAmount, DoubleType())\n",
    "# \"calculate\" is used in spark sql SELECT calculate(...)\n",
    "spark.udf.register(\"calculate\", calculate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3412eb6-101f-4374-bdea-69af80d8d55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- brand_id: long (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- qty: long (nullable = true)\n",
      " |-- discount: long (nullable = true)\n",
      " |-- taxp: long (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "amount is 2242.0\n",
      "amount is 897.92\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+-----+---+--------+----+------+\n",
      "|product_id|product_name|brand_id|price|qty|discount|taxp|amount|\n",
      "+----------+------------+--------+-----+---+--------+----+------+\n",
      "|         1|      iPhone|     100| 1000|  2|       5|  18|2242.0|\n",
      "|         2|      Galaxy|     200|  800|  1|       8|  22|897.92|\n",
      "+----------+------------+--------+-----+---+--------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use udf in data frame\n",
    "from pyspark.sql.functions import col\n",
    "df = orderDf.withColumn(\"amount\", calculate( col(\"price\"), col(\"qty\"), col(\"discount\"), col(\"taxp\")))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33a73ab1-2429-4942-9536-7cb03be403db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temp table/view\n",
    "orderDf.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1375be88-1998-4f1b-8916-42b207c546c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- brand_id: long (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      " |-- qty: long (nullable = true)\n",
      " |-- discount: long (nullable = true)\n",
      " |-- taxp: long (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      "\n",
      "+----------+------------+--------+-----+---+--------+----+------+\n",
      "|product_id|product_name|brand_id|price|qty|discount|taxp|amount|\n",
      "+----------+------------+--------+-----+---+--------+----+------+\n",
      "|         1|      iPhone|     100| 1000|  2|       5|  18|2242.0|\n",
      "|         2|      Galaxy|     200|  800|  1|       8|  22|897.92|\n",
      "+----------+------------+--------+-----+---+--------+----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "amount is 2242.0\n",
      "amount is 897.92\n"
     ]
    }
   ],
   "source": [
    "## now apply udf on spark sql\n",
    "\n",
    "df = spark.sql(\"SELECT *, calculate(price, qty, discount, taxp) as amount from orders\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41af4a4-6b80-4258-ae79-37c33c04abf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIY: spylon in SCALA\n",
    "# DIY: udf for power, calculate\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
